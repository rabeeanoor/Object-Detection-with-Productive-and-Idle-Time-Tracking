{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import time \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading file\n",
    "file = '2nd .mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Rabeea/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2022-10-25 Python-3.9.16 torch-1.11.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# Loading model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s') \n",
    "model.conf = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = file  # source = 0 for webcam\n",
    "\n",
    "video_cap = cv2.VideoCapture(source)\n",
    "if (video_cap.isOpened()== False): \n",
    "  print(\"Error opening video stream or file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29ceb78e640>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret, frame = video_cap.read()\n",
    "frame_w   = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_h   = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "plt.figure(figsize = (20, 8))\n",
    "plt.imshow(frame[...,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI Points\n",
    "points = [\n",
    "    (170, 558),\n",
    "    (596, 636),\n",
    "    (770, 431),\n",
    "    (516, 396)\n",
    "]\n",
    "\n",
    "# Convert points to a numpy array and reshape\n",
    "pts = np.array(points, np.int32)\n",
    "pts = pts.reshape((-1, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(file)\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     #frame = cv2.flip(frame, -1) \n",
    "#     if not ret:\n",
    "#         break\n",
    "    \n",
    "#     #Draw the ROI\n",
    "#     cv2.polylines(frame, [pts], isClosed=True, color=(0, 0, 255), thickness=2, lineType=cv2.LINE_AA)\n",
    "\n",
    "#     results = model(frame)\n",
    "#     labels = results.names\n",
    "#     detections = results.pred[0]\n",
    "    \n",
    "#     for *detected_xyxy, conf, cls in detections:\n",
    "#         if labels[int(cls)] == \"person\": \n",
    "#             label = f\"{labels[int(cls)]} {conf:.2f}\"\n",
    "            \n",
    "#             # Convert bounding box coordinates to integers\n",
    "#             dx1, dy1, dx2, dy2 = map(int, detected_xyxy)\n",
    "            \n",
    "#             # Draw the bounding box of the detected person\n",
    "#             cv2.rectangle(frame, (dx1, dy1), (dx2, dy2), (255, 255, 0), 2)\n",
    "            \n",
    "#             # Put the label above the bounding box\n",
    "#             cv2.putText(frame, label, (dx1, dy1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
    "            \n",
    "#             if all([\n",
    "#                 cv2.pointPolygonTest(pts, (dx1, dy1), False) >= 0,\n",
    "#                 cv2.pointPolygonTest(pts, (dx1, dy2), False) >= 0,\n",
    "#                 cv2.pointPolygonTest(pts, (dx2, dy1), False) >= 0,\n",
    "#                 cv2.pointPolygonTest(pts, (dx2, dy2), False) >= 0,\n",
    "#             ]):\n",
    "#                 cv2.imwrite(\"detected_person.jpg\", frame)\n",
    "#                 break\n",
    "#     # Resize the frame to match the image's resolution\n",
    "#     cv2.imshow(\"Detection\", frame)\n",
    "    \n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# green_box_counter = 0\n",
    "# inside_counter = 0  # Counter for checking consecutive frames a person is inside the ROI\n",
    "# frame_rate = video_cap.get(cv2.CAP_PROP_FPS)\n",
    "# frames_for_2_seconds = int(frame_rate * 2)\n",
    "\n",
    "# cap = cv2.VideoCapture(file)\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "# out = cv2.VideoWriter('output_video.mp4', fourcc, frame_rate, (frame_w, frame_h))\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     # Draw the ROI\n",
    "#     cv2.polylines(frame, [pts], isClosed=True, color=(0, 0, 255), thickness=2, lineType=cv2.LINE_AA)\n",
    "\n",
    "#     results = model(frame)\n",
    "#     labels = results.names\n",
    "#     detections = results.pred[0]\n",
    "    \n",
    "#     for *detected_xyxy, conf, cls in detections:\n",
    "#         if labels[int(cls)] == \"person\": \n",
    "#             label = f\"{labels[int(cls)]} {conf:.2f}\"\n",
    "            \n",
    "#             # Convert bounding box coordinates to integers\n",
    "#             dx1, dy1, dx2, dy2 = map(int, detected_xyxy)\n",
    "            \n",
    "#             # Check if bounding box corners are inside the ROI\n",
    "#             corners_inside = [\n",
    "#                 cv2.pointPolygonTest(pts, (dx1, dy1), False) >= 0,\n",
    "#                 cv2.pointPolygonTest(pts, (dx1, dy2), False) >= 0,\n",
    "#                 cv2.pointPolygonTest(pts, (dx2, dy1), False) >= 0,\n",
    "#                 cv2.pointPolygonTest(pts, (dx2, dy2), False) >= 0\n",
    "#             ]\n",
    "            \n",
    "#             # If any corner is inside the ROI, change color to green, else use light blue\n",
    "#             box_color = (0, 255, 0) if any(corners_inside) else (255, 255, 0)\n",
    "            \n",
    "#             # Draw the bounding box of the detected person\n",
    "#             cv2.rectangle(frame, (dx1, dy1), (dx2, dy2), box_color, 2)\n",
    "            \n",
    "#             # Put the label above the bounding box\n",
    "#             cv2.putText(frame, label, (dx1, dy1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color, 2)\n",
    "            \n",
    "#             # If all corners are inside the ROI, save the frame\n",
    "#             if any(corners_inside):\n",
    "#                 green_box_counter += 1\n",
    "#                 inside_counter += 1\n",
    "\n",
    "#                 if green_box_counter == 1:\n",
    "#                     filename = f\"detected_person_{green_box_counter}.jpg\"\n",
    "#                     cv2.imwrite(filename, frame)\n",
    "                \n",
    "#                 elif inside_counter >= frames_for_2_seconds:\n",
    "#                     filename = \"detected_person_stay.jpg\"\n",
    "#                     cv2.imwrite(filename, frame)\n",
    "#                     inside_counter = 0  # Reset the counter once the fr\n",
    "                \n",
    "#     # Display the frame\n",
    "#     cv2.imshow(\"Detection\", frame)\n",
    "#     out.write(frame)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# out.release()\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######Trying to ADD text box where productive an didle time is mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_box_counter = 0\n",
    "inside_counter = 0  \n",
    "frame_rate = video_cap.get(cv2.CAP_PROP_FPS)\n",
    "frames_for_2_seconds = int(frame_rate * 2)\n",
    "time_per_frame = 1.0 / frame_rate  # time for each frame\n",
    "\n",
    "# Variables for Productive and Idle time tracking\n",
    "productive_time = 0\n",
    "idle_time = 0\n",
    "productive_started = False  # Flag to check if productivity has started\n",
    "\n",
    "cap = cv2.VideoCapture(file)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output_video.mp4', fourcc, frame_rate, (frame_w, frame_h))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    cv2.polylines(frame, [pts], isClosed=True, color=(0, 0, 255), thickness=2, lineType=cv2.LINE_AA)\n",
    "    \n",
    "    results = model(frame)\n",
    "    labels = results.names\n",
    "    detections = results.pred[0]\n",
    "    \n",
    "    person_in_roi = False\n",
    "    \n",
    "    for *detected_xyxy, conf, cls in detections:\n",
    "        if labels[int(cls)] == \"person\":\n",
    "            label = f\"{labels[int(cls)]} {conf:.2f}\"\n",
    "            \n",
    "            dx1, dy1, dx2, dy2 = map(int, detected_xyxy)\n",
    "            \n",
    "            corners_inside = [\n",
    "                cv2.pointPolygonTest(pts, (dx1, dy1), False) >= 0,\n",
    "                cv2.pointPolygonTest(pts, (dx1, dy2), False) >= 0,\n",
    "                cv2.pointPolygonTest(pts, (dx2, dy1), False) >= 0,\n",
    "                cv2.pointPolygonTest(pts, (dx2, dy2), False) >= 0\n",
    "            ]\n",
    "            \n",
    "            box_color = (0, 255, 0) if any(corners_inside) else (255, 255, 0)\n",
    "            cv2.rectangle(frame, (dx1, dy1), (dx2, dy2), box_color, 2)\n",
    "            cv2.putText(frame, label, (dx1, dy1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color, 2)\n",
    "            \n",
    "            if any(corners_inside):\n",
    "                person_in_roi = True\n",
    "                if any(corners_inside):\n",
    "                    green_box_counter += 1\n",
    "                    inside_counter += 1\n",
    "\n",
    "                if green_box_counter == 1:\n",
    "                    filename = f\"detected_person_{green_box_counter}.jpg\"\n",
    "                    cv2.imwrite(filename, frame)\n",
    "                \n",
    "                elif inside_counter >= frames_for_2_seconds:\n",
    "                    filename = \"detected_person_stay.jpg\"\n",
    "                    cv2.imwrite(filename, frame)\n",
    "                    inside_counter = 0  # Reset the counter once the fr\n",
    "                    \n",
    "    # Update Productive and Idle time\n",
    "    if person_in_roi and not productive_started:\n",
    "        productive_started = True\n",
    "        \n",
    "    if productive_started:\n",
    "        productive_time += time_per_frame\n",
    "    else:\n",
    "        idle_time += time_per_frame\n",
    "\n",
    "    # Overlay Productive and Idle times\n",
    "    overlay = frame.copy()\n",
    "    cv2.rectangle(overlay, (0, 0), (frame_w, 40), (255, 255, 255), -1)\n",
    "    alpha = 0.4\n",
    "    cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
    "    cv2.putText(frame, f\"Productive: {productive_time:.2f}s\", (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Idle: {idle_time:.2f}s\", (frame_w - 200, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "    \n",
    "    cv2.imshow(\"Detection\", frame)\n",
    "    out.write(frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.012926676308346"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = './res10_300x300_ssd_iter_140000.caffemodel'\n",
    "# CONFIG_PATH = './deploy.prototxt'\n",
    "\n",
    "# # Load the face detection model.\n",
    "# net = cv2.dnn.readNetFromCaffe(CONFIG_PATH, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = [104, 117, 123]\n",
    "# scale = 1.0\n",
    "# in_width = 300\n",
    "# in_height = 300\n",
    "\n",
    "# # Set the detection threshold for face detections.\n",
    "# detection_threshold = 0.18\n",
    "\n",
    "# # Annotation settings.\n",
    "# font_style = cv2.FONT_HERSHEY_SIMPLEX\n",
    "# font_scale = 0.5\n",
    "# font_thickness = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(file)\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     h = frame.shape[0]\n",
    "#     w = frame.shape[1]    \n",
    "    \n",
    "#     # Convert the image into a blob format.\n",
    "#     blob = cv2.dnn.blobFromImage(frame, scalefactor=scale, size=(in_width, in_height), mean=mean, swapRB=False, crop=False)\n",
    "#     # Pass the blob to the DNN model.\n",
    "#     net.setInput(blob)\n",
    "#     # Retrieve detections from the DNN model.\n",
    "#     detections = net.forward()\n",
    "\n",
    "#     # Process each detection.\n",
    "#     for i in range(detections.shape[2]):\n",
    "#         confidence = detections[0, 0, i, 2]\n",
    "#         if confidence > detection_threshold:\n",
    "\n",
    "#             # Extract the bounding box coordinates from the detection.\n",
    "#             box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "#             (x1, y1, x2, y2) = box.astype('int')\n",
    "\n",
    "#             # Annotate the video frame with the detection results.\n",
    "#             cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "#             label = 'Confidence: %.4f' % confidence\n",
    "#             label_size, base_line = cv2.getTextSize(label, font_style, font_scale, font_thickness)\n",
    "#             cv2.rectangle(frame, (x1, y1 - label_size[1]), (x1 + label_size[0], y1 + base_line), (255, 255, 255), cv2.FILLED)\n",
    "#             cv2.putText(frame, label, (x1, y1), font_style, font_scale, (0, 0, 0))\n",
    "\n",
    "#     # Draw the ROI\n",
    "#     cv2.polylines(frame, [pts], isClosed=True, color=(0, 0, 255), thickness=2, lineType=cv2.LINE_AA)\n",
    "\n",
    "#     results = model(frame)\n",
    "#     labels = results.names\n",
    "#     detections = results.pred[0]\n",
    "\n",
    "#     for *detected_xyxy, conf, cls in detections:\n",
    "#         if labels[int(cls)] == \"person\":\n",
    "#             label = f\"{labels[int(cls)]} {conf:.2f}\"\n",
    "#             dx1, dy1, dx2, dy2 = map(int, detected_xyxy)\n",
    "\n",
    "#             mask = np.zeros_like(frame[:, :, 0])\n",
    "#             cv2.fillPoly(mask, [pts], 255)\n",
    "\n",
    "#             bbox_mask = np.zeros_like(frame[:, :, 0])\n",
    "#             bbox_mask[dy1:dy2, dx1:dx2] = 255\n",
    "\n",
    "#             overlap = np.logical_and(mask, bbox_mask)\n",
    "#             overlap_percentage = np.sum(overlap) / np.sum(bbox_mask)\n",
    "\n",
    "#             inside_roi = overlap_percentage > 0.5  # Adjust this threshold as needed\n",
    "\n",
    "#             box_color = (0, 255, 0) if inside_roi else (255, 255, 0)\n",
    "#             cv2.rectangle(frame, (dx1, dy1), (dx2, dy2), box_color, 2)\n",
    "#             cv2.putText(frame, label, (dx1, dy1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color, 2)\n",
    "\n",
    "#             if inside_roi:\n",
    "#                 cv2.imwrite(\"detected_person.jpg\", frame)\n",
    "\n",
    "#     # Display the detection\n",
    "#     cv2.imshow(\"Detection\", frame)\n",
    "\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "working",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
